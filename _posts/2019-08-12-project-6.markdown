---
layout: default
project-id: 6
date: 2019-08-12
title: SuperGlue
project-date: Aug 2019
img: project-6.jpg
alt: A news story being filmed
link-type: Project Website
link: //media.mit.edu/projects/superglue/overview/

summary: A media digestion tool that cross-analyses verbal and nonverbal cues for presentation, analysis, and summarization of broadcast news.

description: The main objective of this project is to understand the relationship between content and presentation for any given scene and understand the emotive aspects of the same. We want to study the extent to which such presentation affects audiences and see if we can extract the content from its packaging. Moreover, we want to investigate how the presentation of the same content differs from channel to channel. This analysis aimed to measure if such portrayals affect their audiences and contribute to the formation of potentially dangerous echo chambers. <br /> <br /> SuperGlue fuses multiple modalities to create a comprehensive model for the cross-analysis of body language, scene context, and other signals to explore the nature of news on different media outlets. I spearheaded the body language analysis for this project in which we cross-examined hand gestures, facial expressions, and body posture of the newscaster as three dimensions of influence on the overall manner of demonstration. The model used fusion at the decision level for emotion analysis. It used OpenCV for the computer vision aspect and PyTorch for its principal architecture. The components used were a Recurrent Neural Network (RNN) for posture recognition, 3D Convolutional Neural Network (CNN) model for hand gesture recognition, and Azure Media Analyticsâ€™ model for facial emotion recognition.

---
