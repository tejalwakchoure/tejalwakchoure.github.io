---
layout: default
project-id: 6
date: 2019-09-01
title: SuperGlue
project-date: September 2019
img: project-6.jpg
alt: A news story being filmed
category: Affective Analysis, Computer Vision
link-type: Project Website
link: //media.mit.edu/projects/superglue/overview/

summary: A media digestion tool that cross-analyses verbal and nonverbal cues for presentation, analysis, and summarization broadcast news to explore the nature of news on different media outlets. We use this to organize material for presentation, analysis, and summarization. SuperGlue supports other news-related experiments.

description: The main objective of this project is to understand the relationship between content and presentation for any given scene and understand the emotive content of the same. We want to study the extent to which such presentation affects the audiences and see if we can extract the content from its packaging. Moreover, we want to investigate how the presentation of the same content differs from channel to channel. This analysis aimed to measure if such portrayals affect their audiences and contribute to the formation of potentially dangerous echo chambers. <br /> <br /> SuperGlue fuses multiple modalities to create a comprehensive model for the cross-analysis of body language, scene context, and other verbal and nonverbal cues to explore the nature of news on different media outlets. I spearheaded the body language analysis for this project. In this segment, we cross-examined hand gestures, facial expressions, and body posture of the newscaster as three dimensions of influence on the overall manner of demonstration. We discern what aspects of the presentation are the most important for user influence to determine if presentation bias overpowers the content. <br /> <br /> The model used fusion at the decision level for emotion analysis. It used OpenCV for the computer vision aspect and PyTorch for its principal architecture. The models used were <br /> - Recurrent Neural Network (RNN) for posture recognition. <br /> - 3D Convolutional Neural Network (CNN) model with 3 convolutional layers, a pooling layer, and two fully connected layers on image classification for hand gesture recognition. <br /> - Azure Media Analyticsâ€™ model for facial emotion recognition.

---
